{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai \n",
    "import nltk\n",
    "from openai import OpenAI\n",
    "import PyPDF2\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Reads a PDF file and extracts the text.\n",
    "    \n",
    "    :param file_path: Path to the PDF file.\n",
    "    :return: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                page = reader.pages[page_num]\n",
    "                text += page.extract_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_chunk(text, chunk_size):\n",
    "    \"\"\"\n",
    "    Tokenizes the text into sentences and chunks them into groups of approximately chunk_size tokens.\n",
    "    \n",
    "    :param text: The text to be tokenized and chunked.\n",
    "    :param chunk_size: The maximum number of tokens per chunk.\n",
    "    :return: A list of text chunks, each containing approximately chunk_size tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence to count tokens\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        token_length = len(tokens)\n",
    "        \n",
    "        # Check if adding the sentence would exceed the chunk size\n",
    "        if current_length + token_length > chunk_size:\n",
    "            # If so, save the current chunk and start a new one\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        \n",
    "        # Add the sentence to the current chunk\n",
    "        current_chunk.append(sentence)\n",
    "        current_length += token_length\n",
    "    \n",
    "    # Add the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"API_KEY)\n",
    "\n",
    "# Function to get the embedding\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"data\\pdf1.pdf\"  # Replace with the path to your PDF file\n",
    "pdf_text = read_pdf(pdf_path)\n",
    "\n",
    "chunk_size = 1024  # Define your desired chunk size in terms of tokens\n",
    "chunks = tokenize_and_chunk(pdf_text, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"Rex1993\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'chunk' is a list or similar iterable containing your text chunks\n",
    "chunk_embeddings = []\n",
    "for text_chunk in chunks:\n",
    "    # Generate the embedding for each chunk\n",
    "    embedding_vector = get_embedding(text_chunk, model='text-embedding-3-small')\n",
    "    \n",
    "    # Store the chunk and its embedding as a tuple in the list\n",
    "    chunk_embeddings.append((text_chunk, embedding_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL insert statement to store the embedding in vectordatabase\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO vectordatabase (text, embedding)\n",
    "VALUES (%s, %s::vector)\n",
    "RETURNING id;\n",
    "\"\"\"\n",
    "\n",
    "for text_chunk, embedding_vector in chunk_embeddings:\n",
    "    cursor.execute(insert_query, (text_chunk, embedding_vector))\n",
    "    conn.commit()\n",
    "\n",
    "    # Get the ID of the newly inserted row\n",
    "    inserted_id = cursor.fetchone()[0]\n",
    "    print(f\"Inserted embedding with ID: {inserted_id}\")\n",
    "\n",
    "# Close the cursor and connection after all chunks are processed\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index_query = \"\"\"\n",
    "CREATE INDEX ON vectordatabase USING ivfflat (embedding vector_l2_ops) WITH (lists = 100);\n",
    "\"\"\"\n",
    "\n",
    "# Execute the index creation\n",
    "cursor.execute(create_index_query)\n",
    "conn.commit()\n",
    "\n",
    "print(\"Index created successfully.\")\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = get_embedding(\"what is the Endocoder used in transformer\", model='text-embedding-3-small')\n",
    "\n",
    "nn_query = \"\"\"\n",
    "SELECT id, text, embedding\n",
    "FROM vectordatabase\n",
    "ORDER BY embedding <-> %s::vector\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(nn_query, (query_vector,))\n",
    "results = cursor.fetchall()\n",
    "\n",
    "for row in results:\n",
    "    print(f\"ID: {row[0]}, Text: {row[1]}\")\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_chunks = [row[1] for row in results]\n",
    "query_text=\"what is the Endocoder used in transformer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contextual_prompt(query_text, retrieved_chunks):\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuery: {query_text}\\n\\nAnswer:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_with_context(prompt, model=\"gpt-3.5-turbo\"):\n",
    "  response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant. your task is to as give me answer in  layman language as much as possible\"},\n",
    "      {\"role\": \"user\", \"content\": prompt},\n",
    "    \n",
    "    ],temperature=0.7,\n",
    "  )\n",
    "  return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query_text, model='text-embedding-3-small'):\n",
    "    query_vector = get_embedding(query_text, model=model)\n",
    "    \n",
    "    conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"Rex1993\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    nn_query = \"\"\"\n",
    "    SELECT text, embedding\n",
    "    FROM vectordatabase\n",
    "    ORDER BY embedding <-> %s::vector\n",
    "    LIMIT 5;\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.execute(nn_query, (query_vector,))\n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    retrieved_chunks = [row[0] for row in results]\n",
    "    return retrieved_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query_text):\n",
    "    retrieved_chunks = retrieve_relevant_chunks(query_text)\n",
    "    if not retrieved_chunks:\n",
    "        return \"No relevant information found.\"\n",
    "    \n",
    "    prompt = create_contextual_prompt(query_text, retrieved_chunks)\n",
    "    response = generate_response_with_context(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain how transformer works\"\n",
    "\n",
    "response = answer_query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=generate_response_with_context(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query_text):\n",
    "    retrieved_chunks = retrieve_relevant_chunks(query_text)\n",
    "    if not retrieved_chunks:\n",
    "        return \"No relevant information found.\"\n",
    "    \n",
    "    prompt = create_contextual_prompt(query_text, retrieved_chunks)\n",
    "    response = generate_response_with_context(prompt)\n",
    "    return response\n",
    "\n",
    "# Define custom CSS for dark mode\n",
    "# Define custom CSS for a comprehensive dark mode\n",
    "# Define a more refined dark mode CSS\n",
    "\n",
    "\n",
    "# Create the Gradio interface with custom CSS for dark mode\n",
    "iface = gr.Interface(\n",
    "    fn=answer_query,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"RAG Query Answering System\",\n",
    "    description=\"Ask a question and get an AI-generated response based on relevant information retrieved from a database.\",\n",
    "      # Apply refined custom CSS for a dark mode experience\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
